# 推荐系统基础 大数据环境及基础知识
## 1、配置环境

```bash
$ conda create -n reco_sys python=3.6.7
$ conda remove -n reco_sys python=3.6.7
$ source activate reco_sys
$ pip install -r requirements.txt --ignore-installed
$ hostnamectl set-hostname hadoop-master   # 修改hostname

$ yum -y install gcc gcc-c++ kernel-devel   # 安装gcc

$ jps
```

[hadoop主页：http://192.168.19.137:50070](http://192.168.19.137:50070/dfshealth.html#tab-overview)  
[hbase主页：http://192.168.19.137:16010/](http://192.168.19.137:16010/)  
[spark主页：http://192.168.19.137:8088](http://192.168.19.137:8088)

requirements.txt
```
APScheduler==3.5.3
certifi==2019.3.9
chardet==3.0.4
DateTime==4.3
happybase==1.1.0
idna==2.8
jieba==0.39
numpy==1.15.4
pip==19.0.3
ply==3.11
PyHDFS==0.2.1
pytz==2019.1
requests==2.21.0
setuptools==40.8.0
simplejson==3.16.0
six==1.12.0
thriftpy==0.3.9
tzlocal==1.5.1
urllib3==1.24.1
wheel==0.33.1
zope.interface==4.6.0

```

## 2、hadoop
### 2.1、hadoop启动相关命令
```sh
# 启动hadoop命令
cd $HADOOP_HOME/sbin
./start-dfs.sh  # http://192.168.2.137:50070

# 启动yarn
./start-yarn.sh  # http://192.168.2.137:8088
```
### 2.2、运用系统管道命令模拟mapreduce
```sh
# 模拟mapreduce
source activate py365
cat test.txt | python map.py|sort|python red.py
```
### 2.3、真正的mapreduce命令
```sh
# 真正的mapreduce命令
# hadoop-streaming 命令
  STREAM_JAR_PATH="/root/bigdata/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.9.1.jar"    # hadoop streaming jar包所在位置
  INPUT_FILE_PATH_1="/The_Man_of_Property.txt"  #要进行词频统计的文档在hdfs中的路径
  OUTPUT_PATH="/output"                         #MR作业后结果的存放路径
  
  hadoop fs -rm -r -skipTrash $OUTPUT_PATH    # 输出路径如果之前存在 先删掉否则会报错
  
  hadoop jar $STREAM_JAR_PATH \   
  		-input $INPUT_FILE_PATH_1 \ # 指定输入文件位置
  		-output $OUTPUT_PATH \      #指定输出结果位置
  		-mapper "python map.py" \   #指定mapper执行的程序
  		-reducer "python red.py" \  # 指定reduce阶段执行的程序
  		-file ./map.py \            # 通过-file 把python源文件分发到集群的每一台机器上  
  		-file ./red.py

  hadoop jar $STREAM_JAR_PATH -input $INPUT_FILE_PATH_1  -output $OUTPUT_PATH -mapper "/miniconda2/envs/py365/bin/python map.py" -reducer "/miniconda2/envs/py365/bin/python red.py" -file ./map.py -file ./red.py
```
### 2.4、MrJob
#### 2.4.1、MrJob概念
  - ①写一个类继承MRJob
  - ②重写 mapper 和 reducer方法
  - ③在main方法中 调用MRJob.run()方法 开启整个流程

#### 2.4.2、combiner
每一个map都可能会产生大量的本地输出，Combiner的作用就是对map端的输出先做一次合并，以减少在map和reduce节点之间的数据传输量，以提高网络IO性能，是MapReduce的一种优化手段之一。[https://blog.csdn.net/ASN_forever/article/details/81451528](https://blog.csdn.net/ASN_forever/article/details/81451528)
#### 2.4.3、MrJob提交作业的方式
  - 本地测试

    ```python
    python mrjob_wordcount.py 要处理的数据所在位置
    ```

  - 提交到Hadoop集群处理

    ```sh
    # 使用hadoop -r的方式运行
    python mrjob_wordcount.py -r hadoop hdfs:///test.txt -o  hdfs:///output
    ```
  - 如果mapreduce有多个步骤 可以通过steps方法指定

    ```python
    实现steps方法用于指定自定义的mapper，comnbiner和reducer方法
        def steps(self):
            return [
                MRStep(mapper=self.mapper,
                      combiner=self.combiner,
                      reducer=self.reducer_sum),
                MRStep(reducer=self.top_n_reducer)
            ]
    ```
#### 2.4.4、相关代码
- mrjob_wordcount.py
  ```python
  #!/miniconda2/bin/python
  # coding='utf8'
  from mrjob.job import MRJob

  class MRWordFrequencyCount(MRJob):
      def mapper(self, _, line):
          yield "chars", len(line)
          yield "words", len(line.split())
          yield "lines", 1

      def reducer(self, key, values):
          yield key, sum(values)


  class MRWordCount(MRJob):
      def mapper(self, key, line):
          for word in line.split():
              yield word,1
      def reducer(self, word, count):
          yield word, sum(count)

  if __name__ == '__main__':
      # MRWordFrequencyCount().run()
      MRWordCount().run()
  ```
- mrtopN.py
  ```python
  #!/miniconda2/bin/python
  # coding='utf8'
  import sys
  from mrjob.job import MRJob,MRStep
  import heapq

  class TopNWords(MRJob):
      def mapper(self, _, line):
          if line.strip() != "":
              for word in line.strip().split():
                  yield word,1

      #介于mapper和reducer之间，用于临时的将mapper输出的数据进行统计
      def combiner(self, word, counts):
          yield word,sum(counts)

      def reducer_sum(self, word, counts):
          yield None,(sum(counts),word)

      #利用heapq将数据进行排序，将最大的2个取出
      def top_n_reducer(self,_,word_cnts):
          for cnt,word in heapq.nlargest(2,word_cnts):
              yield word,cnt
      
    #实现steps方法用于指定自定义的mapper，comnbiner和reducer方法
      def steps(self):
          return [
              MRStep(mapper=self.mapper,
                    combiner=self.combiner,
                    reducer=self.reducer_sum),
              MRStep(reducer=self.top_n_reducer)
          ]

  def main():
      TopNWords.run()

  if __name__=='__main__':
      main()
  ```
### 2.5 提问
- hadoop计算流程是什么？
- hdfs读写数据的流程是什么样的？
- yarn的节点管理是如何进行的？


### 2.6 遇到的问题
1. Found 1 unexpected arguments on the command line，一般是由于命令写错了，注意空格
2. 如果碰到java.net.NoRouteToHostException: 没有到主机的路由 ，可以尝试将主从节点的防火墙关闭

## 3、hive和HBase
### 3.1、架构及组件
![](./img/hive/hive架构.jpg)
- 组件
   - 用户接口：包括 CLI、JDBC/ODBC、WebGUI。
     - CLI(command line interface)为 shell 命令行
     - JDBC/ODBC 是 Hive 的 JAVA 实现，与传统数据库JDBC 类似
     - WebGUI 是通过浏览器访问 Hive。
     - HiveServer2基于Thrift, 允许远程客户端使用多种编程语言如Java、Python向Hive提交请求
   - 元数据存储：通常是存储在关系数据库如 mysql/derby 中。（一般是存储在mysql中）
     - Hive 将元数据存储在数据库中。
     - Hive 中的元数据包括
       - 表的名字
       - 表的列
       - 分区及其属性
       - 表的属性（是否为外部表等）
       - 表的数据所在目录等。
   - 解释器、编译器、优化器、执行器:完成 HQL 查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划存储在 HDFS 中，并在随后由 MapReduce 调用执行
 - hive和mapreduce的关系
   - Hive 利用 HDFS 存储数据，利用 MapReduce 查询分析数据。  
Hive是**数据仓库工具，没有集群的概念**，如果想提交Hive作业只需要在hadoop集群 Master节点上装Hive就可以了
### 3.2、hive数据模型
- Hive 中所有的数据都存储在 HDFS 中，没有专门的数据存储格式
- 在创建表时指定数据中的分隔符，Hive 就可以映射成功，解析数据。
- Hive 中包含以下数据模型：
  - db：在 hdfs 中表现为 hive.metastore.warehouse.dir 目录下一个文件夹
  - table：在 hdfs 中表现所属 db 目录下一个文件夹
  - external table：数据存放位置可以在 HDFS 任意指定路径
  - partition：在 hdfs 中表现为 table 目录下的子目录
  - bucket：在 hdfs 中表现为同一个表目录下根据 hash 散列之后的多个文件

### 3.3、hive启动命令
```sh
# 启动docker 
service docker start

# 通过docker 启动mysql
docker start mysql

# 启动 hive的metastore元数据服务
hive --service metastore &

# 启动hive
hive

# 用户名密码
# MySQL root 密码 password         
# hive用户 密码 hive
```


## 4、Spark

### 4.1、启动Spark集群

- 进入到$SPARK_HOME/sbin目录
  ```sh
  cd $SPARK_HOME/sbin
  ```
  - 启动Master	
    ```sh
    ./start-master.sh -h 192.168.19.137
    ```
  - 启动Slave
    ```sh
    ./start-slave.sh spark://192.168.19.137:7077
    ```
  - jps查看进程
    ```sh
    27073 Master
    27151 Worker
    ```
  - 关闭防火墙
    ```sh
    systemctl stop firewalld
    ```
  - 通过SPARK WEB UI查看Spark集群及Spark
    - http://192.168.19.137:8080/  监控Spark集群
    - http://192.168.19.137:4040/  监控Spark Job
  - 使用jupyternotebook进行开发
    ```sh
    jupyter notebook --ip 0.0.0.0 --allow-root
    ```
### 4.*、遇到的问题
#### 4.*.1 读取本地文件和hdfs的文件
在不同的启动模式下，加载文件时的路径写法是不一样的，对于local模式下，默认就是读取本地文件，而在standlone或者yarn-client,或者cluster模式下，默认读的都是hdfs文件系统，这几种模式下很难读取本地文件（这是很显然的事情，但你可以通过指定节点的文件服务曲线救国）。

下面的代码在local模式下有效，在其它模式下无效：

var theP1 = sc.textFile("file:///usr/test/people.json")  //读取本地
var theP2 = sc.textFile("hdfs://master.hadoop/user/root/test/test/people.json") //读取hdfs文件

下面的代码在非local模式下，都是读取的hdfs,file://模式无效.

var theP1 = sc.textFile("/usr/test/people.json") 
var theP2 = sc.textFile("/user/root/test/test/people.json")

下面这个语句在几种模式下都有效

var theP2 = sc.textFile("hdfs://master.hadoop/user/root/test/test/people.json") //读取hdfs文件

在非local模式下等同于

var theP2 = sc.textFile("/user/root/test/test/people.json")

#### 4.*.2 使用jupyter notebook 利用spark读取hdfs上的数据时，一直获取不到资源

```sh
...
20/01/31 23:42:01 WARN scheduler.TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
...
```
查看spark主页面，发现没有执行的worker

这个情况，可以先重新启动hadoop环境，然后再启动jupyter notebook，然后运行代码

附一些脚本：
```sh
######################################################################### 
# 用来启动所有hadoop生态的脚本
# File Name: start.sh
#########################################################################

#!/bin/bash
/root/bigdata/spark/sbin/start-all.sh
start-hbase.sh
/root/bigdata/hadoop/sbin/start-all.sh
```

```sh
######################################################################### 
# 用来启动所有spark生态的脚本
# File Name: /root/bigdata/spark/sbin/start-all.sh
#########################################################################

#!/usr/bin/env bash
if [ -z "${SPARK_HOME}" ]; then
  export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"
fi

# Load the Spark configuration
. "${SPARK_HOME}/sbin/spark-config.sh"

# Start Master 这里-h指定了主机ip
"${SPARK_HOME}/sbin"/start-master.sh -h 192.168.19.137

# Start Workers 指定worker的url spark://192.168.19.137:7077
"${SPARK_HOME}/sbin"/start-slaves.sh spark://192.168.19.137:7077
```
**需要注意**：hadoop-slave1和hadoop-slave2都需要执行如下命令：
```sh
# 切换到两台从机上进行执行如下命令
cd /root/bigdata/spark/sbin
./start-slave.sh spark://192.168.19.137:7077
```


```sh
######################################################################### 
# 用来启动hadoop相关的脚本，yarn，hdfs，hadoop
# File Name:/root/bigdata/hadoop/sbin/start-all.sh
#########################################################################

#!/usr/bin/env bash
echo "This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh"

bin=`dirname "${BASH_SOURCE-$0}"`
bin=`cd "$bin"; pwd`

DEFAULT_LIBEXEC_DIR="$bin"/../libexec
HADOOP_LIBEXEC_DIR=${HADOOP_LIBEXEC_DIR:-$DEFAULT_LIBEXEC_DIR}
. $HADOOP_LIBEXEC_DIR/hadoop-config.sh

# start hdfs daemons if hdfs is present
if [ -f "${HADOOP_HDFS_HOME}"/sbin/start-dfs.sh ]; then
  "${HADOOP_HDFS_HOME}"/sbin/start-dfs.sh --config $HADOOP_CONF_DIR
fi

# start yarn daemons if yarn is present
if [ -f "${HADOOP_YARN_HOME}"/sbin/start-yarn.sh ]; then
  "${HADOOP_YARN_HOME}"/sbin/start-yarn.sh --config $HADOOP_CONF_DIR
fi

```

## [附]
如何扩展磁盘空间：
1. https://www.cnblogs.com/lfxiao/p/9519759.html
2. https://blog.csdn.net/wh445306/article/details/100932430


# 黑马头条推荐系统

docker
```bash
$ systemctl start docker  #启动         
$ sudo systemctl daemon-reload 守护进程重启  
$ systemctl restart  docker # 重启docker服务   
$ sudo service docker restart  # 重启docker服务  
$ service docker stop   #关闭docker   
$ systemctl stop docker  # 关闭docker  
```

```bash
$ docker exec -it mysql bash
$ mysql -uroot -p   # 密码是password
```


```mysql
show databases;

```


hive
```bash
$ hive --service metastore &  # 启动
```


## 面试经历
作者：牛客网
链接：https://zhuanlan.zhihu.com/p/65034443
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

背景介绍：本人经历几乎没有什么实践项目，大部分是算法相关的工作，研究生阶段主要做的是推荐系统，涉及矩阵分解、图嵌入等，所以问题很多是和我方向相关的，大家可以有选择地参考一下~一面：几乎全部是项目相关的：1、说说矩阵分解2、围绕LLE来问：LLE全称是什么；简述LLE和PCA的特点和区别；LLE里面涉及的图拉普拉斯有没有了解（应该问的是LE：Laplace Eigenmaps）（一定要了解相近的一类对比算法）3、整体代码的实现（一定要有条理地说清楚啊）；deepwalk是手写还是工具包，有没有用numpy；图嵌入的训练集是什么，矩阵分解的训练集是什么4、简述word2vec；说说滑动窗口大小以及负采样个数的参数设置以及设置的比例；怎么衡量学到的embedding的好坏5、是否了解图卷积6、说说推荐系统算法大概可以分为哪些种类：（1）基于内容；（2）基于协同过滤：基于内存（UB IB）；基于模型（MF）二面：1、推导LR2、图结构是怎么存储的？利用你所做的这个图结构实现深度/广度优先遍历，格式是：def find_path(graph, root, destination)深度优先遍历用栈结构实现；广度优先遍历用队列结构实现3、聊到了宏观会问到的业务上的问题：如果图表只存储了学校这片区域的中心点，但是我们下单的宿舍地址不在中心点附近，怎么去确定这个具体位置？说：可以遍历走过该地址的外卖员的轨迹，大量相交的交点大概率是具体位置；还问，如果要给外卖员分配订单，怎么去分配？从外卖员到下单地址的距离远近，下单的紧急程度，外卖员正在派送的位置与下一个要派送的位置是否顺路（不可以时东时西）三面：1、详细描述工作，画出来整体框架2、工作最大创新点，在代码实现方面遇到的难点3、看你对比的都是传统的或者是基于图的推荐算法，有没有尝试过对比一下或者有没有了解其他不同数据源的深度学习算法？4、说到上面提到了attention机制，问了怎么看待attention机制，为什么有这么多工作去使用它5、除了优化模型，还可以从什么方面去取得更好的性能：说了特征工程的处理，GBDT得到feature importance取topk贡献较大的特征作为模型输入6、上面说到的特征处理，提到了会筛选出来特别的节日来单独处理，问：为什么要把平常日、周末、节假日分开处理7、怎么去规划工作几年中的小目标四面：1、推导SVM公式，挨个步骤说清楚，我说错了y的取值范围，应该是{ 1，-1}；没说清楚函数间隔和几何间隔的物理含义2、问了满二叉树和完全二叉树，大概画了一下；问了红黑树，说没学过，没有接着问了3、问了随机森林有了解吗？知道里面的有放回的采样方法吗？后面问了个数学问题：给定n个小球，有放回地采样。当n趋向于无穷的时候，某小球不被取到的概率是多少？--------------------以上是全部流程啦，其中一面是电话面的，后三面是去到美团现场面的（时间会提前跟我们沟通好的），一个下午面完，感觉效率很高，全程感觉也比较好，面试官会根据我们经历，顺着我们的话去深挖，也会结合业务去一块讨论。收获满满的一次面试~ 希望可以对大家能有一点帮助，一起加油鸭！~~~与作者交流：https://www.nowcoder.com/discuss/187307更多笔经面经：https://www.nowcoder.com/discuss?order=0&type=2
